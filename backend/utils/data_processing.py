import os
import json
import numpy as np
import pandas as pd
import logging
from werkzeug.utils import secure_filename
from datetime import datetime

logger = logging.getLogger(__name__)

ALLOWED_EXTENSIONS = {'csv', 'npz', 'json'}
UPLOAD_FOLDER = '/app/uploads'

def allowed_file(filename):
    """Check if file extension is allowed"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def ensure_upload_folder():
    """Ensure upload folder exists"""
    if not os.path.exists(UPLOAD_FOLDER):
        os.makedirs(UPLOAD_FOLDER)

def save_uploaded_file(file):
    """Save uploaded file and return the path"""
    try:
        ensure_upload_folder()
        
        if file.filename == '':
            return None, "No file selected"
        
        if not allowed_file(file.filename):
            return None, "File type not allowed"
        
        filename = secure_filename(file.filename)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{filename}"
        filepath = os.path.join(UPLOAD_FOLDER, filename)
        
        file.save(filepath)
        logger.info(f"File saved: {filepath}")
        return filepath, None
        
    except Exception as e:
        logger.error(f"Error saving file: {e}")
        return None, str(e)

def process_csv_file(filepath):
    """Process CSV file and extract relevant data"""
    try:
        df = pd.read_csv(filepath)
        logger.info(f"CSV loaded with shape: {df.shape}")
        
        # Expected columns for crop data
        expected_columns = ['ndvi', 'temperature', 'humidity', 'soil_moisture', 'ph']
        
        # Check if we have numeric data
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        
        if len(numeric_columns) == 0:
            return None, "No numeric data found in CSV"
        
        # Use available numeric columns or create synthetic data
        processed_data = {}
        
        # Try to find NDVI data
        ndvi_columns = [col for col in df.columns if 'ndvi' in col.lower()]
        if ndvi_columns:
            processed_data['ndvi'] = df[ndvi_columns[0]].values
        elif len(numeric_columns) > 0:
            # Use first numeric column as NDVI proxy
            processed_data['ndvi'] = df[numeric_columns[0]].values
        else:
            # Generate synthetic NDVI data
            processed_data['ndvi'] = np.random.uniform(0.2, 0.8, len(df))
        
        # Extract other features if available
        for col in ['temperature', 'humidity', 'soil_moisture', 'ph']:
            matching_cols = [c for c in df.columns if col.lower() in c.lower()]
            if matching_cols:
                processed_data[col] = df[matching_cols[0]].values
            elif col in numeric_columns:
                processed_data[col] = df[col].values
        
        # If we don't have enough features, create synthetic ones
        for col in ['temperature', 'humidity', 'soil_moisture', 'ph']:
            if col not in processed_data:
                if col == 'temperature':
                    processed_data[col] = np.random.uniform(15, 35, len(df))
                elif col == 'humidity':
                    processed_data[col] = np.random.uniform(30, 90, len(df))
                elif col == 'soil_moisture':
                    processed_data[col] = np.random.uniform(20, 80, len(df))
                elif col == 'ph':
                    processed_data[col] = np.random.uniform(5.5, 8.5, len(df))
        
        # Create feature matrix
        features = []
        for col in ['ndvi', 'temperature', 'humidity', 'soil_moisture', 'ph']:
            if col in processed_data:
                features.append(processed_data[col])
        
        feature_matrix = np.column_stack(features) if features else None
        
        return {
            'data': processed_data,
            'features': feature_matrix,
            'shape': feature_matrix.shape if feature_matrix is not None else (0, 0),
            'columns': list(processed_data.keys()) if processed_data else []
        }, None
        
    except Exception as e:
        logger.error(f"Error processing CSV: {e}")
        return None, str(e)

def process_npz_file(filepath):
    """Process NPZ file and extract relevant data"""
    try:
        data = np.load(filepath)
        logger.info(f"NPZ loaded with keys: {list(data.keys())}")
        
        processed_data = {}
        
        # Try to find common array names
        for key in data.keys():
            array = data[key]
            if array.ndim <= 2:  # Only process 1D or 2D arrays
                processed_data[key] = array
        
        # Extract features for model prediction
        feature_arrays = []
        
        # Look for NDVI data
        ndvi_keys = [k for k in processed_data.keys() if 'ndvi' in k.lower()]
        if ndvi_keys:
            ndvi_data = processed_data[ndvi_keys[0]]
            if ndvi_data.ndim == 1:
                feature_arrays.append(ndvi_data.reshape(-1, 1))
            else:
                feature_arrays.append(ndvi_data)
        
        # Include other numeric arrays
        for key, array in processed_data.items():
            if 'ndvi' not in key.lower() and array.dtype in [np.float32, np.float64, np.int32, np.int64]:
                if array.ndim == 1:
                    feature_arrays.append(array.reshape(-1, 1))
                elif array.ndim == 2 and array.shape[1] <= 10:  # Reasonable feature count
                    feature_arrays.append(array)
        
        # Combine features
        if feature_arrays:
            # Ensure all arrays have same number of samples
            min_samples = min(arr.shape[0] for arr in feature_arrays)
            feature_arrays = [arr[:min_samples] for arr in feature_arrays]
            feature_matrix = np.concatenate(feature_arrays, axis=1)
        else:
            # Create synthetic features if no suitable data found
            sample_size = 100
            feature_matrix = np.random.rand(sample_size, 5)  # 5 features: NDVI, temp, humidity, soil moisture, pH
            processed_data = {
                'ndvi': feature_matrix[:, 0],
                'temperature': feature_matrix[:, 1] * 20 + 15,  # 15-35Â°C
                'humidity': feature_matrix[:, 2] * 60 + 30,     # 30-90%
                'soil_moisture': feature_matrix[:, 3] * 60 + 20, # 20-80%
                'ph': feature_matrix[:, 4] * 3 + 5.5             # 5.5-8.5
            }
        
        return {
            'data': processed_data,
            'features': feature_matrix,
            'shape': feature_matrix.shape,
            'columns': list(processed_data.keys())
        }, None
        
    except Exception as e:
        logger.error(f"Error processing NPZ: {e}")
        return None, str(e)

def process_json_file(filepath):
    """Process JSON file and extract relevant data"""
    try:
        with open(filepath, 'r') as f:
            data = json.load(f)
        
        logger.info(f"JSON loaded with keys: {list(data.keys()) if isinstance(data, dict) else 'List of items'}")
        
        processed_data = {}
        
        # Handle different JSON structures
        if isinstance(data, dict):
            # Dictionary format
            for key, value in data.items():
                if isinstance(value, (list, np.ndarray)):
                    processed_data[key] = np.array(value)
                elif isinstance(value, (int, float)):
                    processed_data[key] = np.array([value])
        elif isinstance(data, list):
            # List of records format
            if data and isinstance(data[0], dict):
                # Convert list of dicts to dict of lists
                keys = data[0].keys()
                for key in keys:
                    values = [record.get(key, 0) for record in data]
                    try:
                        processed_data[key] = np.array([float(v) for v in values])
                    except (ValueError, TypeError):
                        # Skip non-numeric data
                        continue
        
        # Create feature matrix
        feature_arrays = []
        
        # Look for NDVI
        ndvi_keys = [k for k in processed_data.keys() if 'ndvi' in k.lower()]
        if ndvi_keys:
            ndvi_data = processed_data[ndvi_keys[0]]
            feature_arrays.append(ndvi_data.reshape(-1, 1))
        
        # Add other numeric features
        for key, array in processed_data.items():
            if 'ndvi' not in key.lower() and array.dtype in [np.float32, np.float64, np.int32, np.int64]:
                feature_arrays.append(array.reshape(-1, 1))
        
        if feature_arrays:
            # Ensure all arrays have same length
            min_samples = min(arr.shape[0] for arr in feature_arrays)
            feature_arrays = [arr[:min_samples] for arr in feature_arrays]
            feature_matrix = np.concatenate(feature_arrays, axis=1)
        else:
            # Generate synthetic data if no suitable data found
            sample_size = 50
            feature_matrix = np.random.rand(sample_size, 5)
            processed_data = {
                'ndvi': feature_matrix[:, 0],
                'temperature': feature_matrix[:, 1] * 20 + 15,
                'humidity': feature_matrix[:, 2] * 60 + 30,
                'soil_moisture': feature_matrix[:, 3] * 60 + 20,
                'ph': feature_matrix[:, 4] * 3 + 5.5
            }
        
        return {
            'data': processed_data,
            'features': feature_matrix,
            'shape': feature_matrix.shape,
            'columns': list(processed_data.keys())
        }, None
        
    except Exception as e:
        logger.error(f"Error processing JSON: {e}")
        return None, str(e)

def process_uploaded_data(filepath):
    """Process uploaded data file based on its extension"""
    try:
        if not os.path.exists(filepath):
            return None, "File not found"
        
        _, ext = os.path.splitext(filepath)
        ext = ext.lower()
        
        if ext == '.csv':
            return process_csv_file(filepath)
        elif ext == '.npz':
            return process_npz_file(filepath)
        elif ext == '.json':
            return process_json_file(filepath)
        else:
            return None, f"Unsupported file type: {ext}"
    
    except Exception as e:
        logger.error(f"Error processing uploaded data: {e}")
        return None, str(e)

def validate_processed_data(processed_data):
    """Validate that processed data has required structure"""
    if not processed_data:
        return False, "No processed data"
    
    if 'features' not in processed_data:
        return False, "No features found in processed data"
    
    features = processed_data['features']
    if features is None or features.size == 0:
        return False, "Feature matrix is empty"
    
    if len(features.shape) != 2:
        return False, "Feature matrix must be 2-dimensional"
    
    if features.shape[0] == 0 or features.shape[1] == 0:
        return False, "Feature matrix has zero dimensions"
    
    return True, "Data is valid"

def cleanup_file(filepath):
    """Clean up uploaded file"""
    try:
        if os.path.exists(filepath):
            os.remove(filepath)
            logger.info(f"Cleaned up file: {filepath}")
    except Exception as e:
        logger.error(f"Error cleaning up file: {e}")
